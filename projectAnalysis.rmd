---
title: "Practical-Machine-Learning-Assignment"
author: "Bently"
date: "2/5/2021"
output: html_document
---

```{r warning=FALSE, message=FALSE}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(knitr)
library(ggplot2)
library(gbm)

knitr::opts_chunk$set(echo = TRUE)
```

### Project Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here:](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).


### Our Overall Goal
The goal of your project is to predict the manner in which they did the exercise. This is the “classe” variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.


## Getting and Load Data
The training data for this project are available [here:](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The test data are available [here:](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The data for this project come from this [source:](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har). 
If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing 
their data to be used for this kind of assignment.
```{r echo=TRUE, results= 'asis', warning=FALSE}
set.seed(54321)

trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))

```

## Partioning/Split the Training Set into two
In real life, simple models often beat complex ones, because they can generalize much better. We will do a random 60:40 
split in our data set (60% will be for training models, 40% to evaluate them)
```{r echo=TRUE, results= 'asis', warning=FALSE}

set.seed(123) # For reproducibility; 123 has no particular meaning
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)

```


## Cleaning the data
Remove the NearZeroVariance variables

caret::nearZeroVar diagnoses predictors that have one unique value (i.e. are zero variance predictors) or predictors that are have both of the following characteristics: they have very few unique values relative to the number of samples and the ratio of the frequency of the most common value to the frequency of the second most common value is large.
```{r echo=TRUE, results= 'asis', warning=FALSE}
nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]

nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]

#Remove the first column of the Train data set
myTraining <- myTraining[c(-1)]

#Clean variables with more than 60% NA
trainingV3 <- myTraining
for(i in 1:length(myTraining)) {
    if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .7) {
        for(j in 1:length(trainingV3)) {
            if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) == 1)  {
                trainingV3 <- trainingV3[ , -j]
            }   
        } 
    }
}

# Set back to the original variable name
myTraining <- trainingV3
rm(trainingV3)
```


## Feature Selection
In order to select the relevant features, the various variables were plotted graphically.  To begin, the variables were plotted a few at a time using boxplots in R's <TT>featurePlot()</TT> function, like so:
```{r}
featurePlot(x = myTraining[,8:10], y = myTraining$classe, plot = "box")
```

At this point, the plots were visually inspected.  Any variable where the boxes for a single variable had some significant differences were then further inspected via a stacked histogram, such as this one for the accel_belt_z variable above:

```{r, warning = FALSE}
ggplot(data = myTraining, aes(x = accel_belt_z, fill = classe)) + geom_histogram()
```

Those that were determined to be potentially useful were then noted, and ultimately used in the final model.

### Transform the Test and Testing data sets
Here is one of the golden rules of machine learning and modeling in general: models are built using training data, and evaluated on testing data. The reason is over-fitting: most models' accuracy can be artificially increased to a point where they "learn" every single detail of the data used to build them; unfortunately, it usually means they lose the capability to generalize. That's why we need unseen data (i.e., the testing set): if we over-fit the training data, the performance on the testing data will be poor.
```{r echo=TRUE, results= 'asis', warning=FALSE}
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58])  # remove the classe column
myTesting <- myTesting[clean1]         # allow only variables in myTesting that are also in myTraining
testing <- testing[clean2]             # allow only variables in testing that are also in myTraining

dim(myTesting)

#Coerce the data into the same type
for (i in 1:length(testing) ) {
    for(j in 1:length(myTraining)) {
        if( length( grep(names(myTraining[i]), names(testing)[j]) ) == 1)  {
            class(testing[j]) <- class(myTraining[i])
        }      
    }      
}

# To get the same class between testing and myTraining
testing <- rbind(myTraining[2, -58] , testing)
testing <- testing[-1,]

```


## Prediction with Decision Trees
A decision tree (also known as regression tree for continuous outcome variables) is a simple and popular machine learning algorithm, with a few interesting advantages over linear models: they make no assumptions about the relation between the outcome and predictors (i.e., they allow for linear and non-linear relations); the interpretability of a decision tree could not be higher - at the end of the process, a set of rules, in natural language, relating the outcome to the explanatory variables, can be easily derived from the tree.
```{r echo=TRUE, results= 'asis', warning=FALSE}
set.seed(123) #For reproducibility; 123 has no particular meaning
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")

fancyRpartPlot(modFitA1)
```


### Confusion Matrix
Predicting between pairs produces categorical output: -1, 0, or 1. A [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) counts how many times the predicted category mapped to the various true categories.
```{r echo=TRUE, results= 'asis', warning=FALSE}
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
cmtree <- confusionMatrix(predictionsA1, myTesting$classe)
cmtree

```


### Decision Tree Confusion Matrix
```{r echo=TRUE, results= 'asis', warning=FALSE}
plot(cmtree$table, col = cmtree$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(cmtree$overall['Accuracy'], 4)))

```


## Prediction with Random Forests
What if, instead of growing a single tree, we grow many (ranging from a few hundred to a few thousand), and introduce some sources of randomness, so that each tree is most likely different from the others? What we get is a random forest

* How many trees are needed to reach the minimum error estimate? 

* This is a simple problem; it appears that about 100 trees would be enough. 

Some of most interesting characteristics of random forests are:

* They do not over-fit.

* There is no need for cross-validation.

* We can grow as many tree as we want (the limit is the computational power).

* Although we usually improve accuracy, it comes at a cost: interpretability.
```{r echo=TRUE, results= 'asis', warning=FALSE}
set.seed(123)  #For reproducibility; 123 has no particular meaning
modFitB1 <- randomForest(classe ~ ., data=myTraining, importance = TRUE, ntree=100)
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf

plot(modFitB1)
```


```{r echo=TRUE, results= 'asis', warning=FALSE}

plot(cmrf$table, col = cmtree$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))

```



## Prediction with Generalized Boosted Regression
Gradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many 
domains. Whereas random forests build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak 
successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees 
produce a powerful “committee” that are often hard to beat with other algorithms

We will use 5-fold cross validation to estimate accuracy.

This will split our data set into 5 parts, train in 4 and test on 1 and release for all combinations of train-test splits. 
We will also repeat the process 1 times for each algorithm with different splits of the data into 10 groups, in an effort to 
get a more accurate estimate. [Also, getting the gbm info off of:](https://code.google.com/p/gradientboostedmodels/) 
```{r echo=TRUE, results= 'asis', warning=FALSE}
set.seed(123)  #For reproducibility; 123 has no particular meaning

# Run algorithms using 5-fold cross validation repeated 1 times
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 1)

# Using caret with the default grid to optimize tune parameters automatically
# GBM Tuning parameters:
# n.trees (# Boosting Iterations)
# interaction.depth (Max Tree Depth)
# shrinkage (Shrinkage)
# n.minobsinnode (Min. Terminal Node Size)

gbmFit1 <- train(classe ~ .
                 , data = myTraining
                 , method = "gbm"
                 , trControl = fitControl 
                 , verbose = FALSE
                )




gbmFinMod1 <- gbmFit1$finalModel

gbmPredTest <- predict(gbmFit1, newdata=myTesting)
gbmAccuracyTest <- confusionMatrix(gbmPredTest, myTesting$classe)
gbmAccuracyTest

plot(gbmFit1, ylim=c(0.9, 1))
plot(gbmFit1, metric = "Kappa")
plot(gbmFit1, plotType = "level")
resampleHist(gbmFit1)
```


## Predicting Results on the Test Data
Random Forests gave an Accuracy in the Testing data set of 99.81%, which was more accurate that what I got from the 
Decision Trees or GBM. 

* The expected out-of-sample error is 100% -99.81% = 0.19%.
```{r echo=TRUE, results= 'asis', warning=FALSE}
predictionB2 <- predict(modFitB1, testing, type = "class")
predictionB2


# Write the results to a text file for submission
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}
```













